================================================================== 07/28 ==================================================================
선수들 고유 id를 알아내는게 급선무였다. id를 알아야 바꿔가면서 페이지에 접속해서 정보들을 얻어올 수 있기 때문이다.
선수 id를 처음에 뜬 '지네딘 지단'으로 먼저 알아내고, 9자리 숫자로 이루어졌다는걸 알고 0부터 쭉 훑으면서 정보가 없으면 skip하는 식으로 하려고 했다.
그러나 open API에 선수들 spid정보가 다 들어있다는 걸 알면서, 9자리중 앞 3자리는 시즌에 대한 id, 뒷 6자리는 선수에 대한 id임을 알았다.
7/29 아침 현재 기준으로 총 선수는 약 38423명임을 spid.json 파일의 length를 통해서 알 수 있었다.
나머지는 노가다였는데, id를 알고 접속한다음 웬만한 정보들은 다 받아올 수 있었다.
하나하나의 값을 태그로 다 얻어서 와도 되지만, 34개로 이루어진 스탯은 2차원의 박스안에 나란히 데이터가 정렬되있음을 알고, 태그를 for문으로 바꿔가면서 쭉 읽어올 수 있게 되었다.
그런데 스탯을 읽어올 때 문제가 발생했다. 이게 웹페이지에는 제일 기본의 스탯, 즉 0카일때의 스탯을 저장해놓는 것이다.
예를 들어 101000001 id를 가진 '데이비드 시먼'을 기준으로 '드리블' 스탯은 52로 나온다. 그러나 0카이면 3을 빼서 49라는 값이 html에 저장이 되어있고, 이거를 selector 이름에 (능력치 수치마다 색깔이 바뀌기 때문에) 'over 50' 'over 60' 이런식으로 달아놔서, 페이지에서 selector를 복사해서 드리블 스탯을 찾으면 원본 html에는 'over 50' 태그에 드리블이 없기 때문에 (드리블 0카 기준 스탯이 49니까) 안나오는 것이다.
그래서 그냥 그 상위 태그로 들어가서 스탯이름, 스탯수치 가 같이 나오게 크롤링한다음에, 파이썬 string에서 제공하는 split으로 공백없애고 하나씩 저장하는 식으로 해결.
+ html 파싱에 종류가 여러가지 있는데, 매개변수로 "html.parser", "html5lib", "lxml" 등이 있다. 속도가 제일 빠르다고 평가되는 "lxml"로 프로그래밍했다.


================================================================== 07/29 ==================================================================
이거 진짜 개빡쳤는데 선수 현재시세정보가 이게 페이지 로드된다음에 javascript로 실행되서 데이터가 산출되서 나오는거라 그냥 html에는 그 정보가 없는거임.
그래서 알아보니까 결국 동적 크롤링을 해야된다는데. 동적 크롤링 정보가 selenium에 대한거 밖에 없어서 덕분에 selenium써서 일단 하긴함. 덕분에 공부함. 근데 너무 느린거임. 거의 하나당 8초가 걸리니까 40000명 육박하는 데이터 뽑으려면 사실상 시간이 말이안됨. 진짜 미친듯이 연구하다가 개발자도구에서 network 탭 들어간다음에 xhr파일에 PlayerPriceGraph라는 파일을 찾음.
이거 주소로 들어가면 이상한 기본페이지만 떠서 또 정보가 안받아져옴. 적절한 header가 있어야 하는거 같이 그분이 적어놔서 따라해도 안됬음.
근데 가만보니까 request method가 그분이 한 페이지는 get인데 나는 post로 되어있는 것. 그래서 request.get을 안하고 .post로 하면서 data로 선수 id랑 그거 주니까 바로 나왔음.


================================================================== 07/30 ==================================================================
getPlayerPrice.py를 조금 손봐서, spid.json에서 테스트하고 싶은 정도만큼 가격 받아온 다음에 playerPriceDB.json 으로 출력되게 했다.
1000명 정도 길이로 테스트를 하니까 웹에서 데이터 긁어오기 + json파일 writing 을 합쳐서 약 34초의 시간이 걸렸다. 두번째 시도에는 13초의 시간이 걸렸다 .... ?


================================================================== 08/03 ==================================================================
플레이어 선호포지션이 여러개인 경우도있고, 한개인 경우도 있어서 해결하는 알고리즘 test.py에 작성


================================================================== 08/05 ==================================================================
전반적으로 함수 리패키징이랑 코드들 정리를 함. selector.py, getters.py, getPlayerBasicInfo.py를 만들어서 각각 필요한 정보들을 넣어놓고 from * import * 구문으로 필요한 함수들을 가지고와서 총 정리를 했음.
추가적으로 포지션별 스탯들이랑, 선호포지션 갯수 대로 받는 부분, 클럽 경력 까지 다 받아올 수 있게 되었다. 일단 정리는 해야할 것 같음.
선호포지션의 오버롤은 포지션별 오버롤에서 일치하면 가져오는걸로 마무리할 듯.


================================================================== 08/06 ==================================================================
header = {"X-Requested-With" : "XMLHttpRequest"}
이놈이 핵심이었다. 팀컬러페이지에서 정보를 가져오는 부분을 프로그래밍함. 역시 가격 가져올때랑 똑같이 엄청 안됐는데, 헤더에 저부분이 들어가니까 바로 해결되었음.
내일은 이부분들 다 받아서 정리해보는걸로 할듯. 그리고 spid에서 s는 season, p는 player인듯. 시즌이 다른데 같은 선수들은 pid(뒤 6자리)가 모두 같음.


================================================================== 08/07 ==================================================================
nations.json 파일 생성. 전체 국가 리스트 및 대륙별 국가 리스트 생성해서 넣어놓음.
tag로 element를 찾은 뒤에는 [속성값(attribute)] 을 통해 찾아올 수가 있었음. 마지막 태그 안에 data-no = 2~7 까지의 값을 대륙마다 가져서 그걸로 대륙별로 나눠놓음.
총 209개의 국가가 있었고, 국가는 있지만 소속 선수는 없는 경우들도 있었음. 209개인걸로 봐서 거의 모든 국가를 일단 다 넣어놓긴 한거같음.
sort() 함수를 통해서 가나다순으로 정렬해서 저장해놓음.
포지션별 index 정보는 selectors.py에 저장해놨는데. 이거 api로 받은 sposition.json 파일에 다 저장되어있었음.


================================================================== 08/07 ==================================================================
- 주발, 약발 부분 foot=[3,5], mainfoot = R 이런식으로 수정
- 특성부분 길이가 1일때는 그냥 string으로 들어가고 개수가 많으면 array로 들어가서 그러면 object 형태가 달라지니까 길이가 1이어도 string으로 되게 만듬
- getstat 부분 구현해서 다 받아오게 끔 기능 넣음
- 멀티프로세싱으로 pool을 이용해서 4개의 프로세스를 돌아가게 만들어서 속도문제 개선